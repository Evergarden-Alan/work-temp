{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938b6000-9d45-49bc-b106-4e138863d5f8",
   "metadata": {},
   "source": [
    "# Transformer&Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab43f15-71f4-4c22-ba63-f91d4de74c7b",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26fda26c-bd72-43e3-97fb-1cec2a0b447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "'''Attention Is All You Need'''\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab-1)\n",
    "\n",
    "        self.postion_embedding = Positional_Encoding(config.embed, config.pad_size, config.dropout, config.device)\n",
    "        self.encoder = Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            copy.deepcopy(self.encoder)\n",
    "            # Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "            for _ in range(config.num_encoder)])\n",
    "\n",
    "        self.fc1 = nn.Linear(config.pad_size * config.dim_model, config.num_classes)\n",
    "        # self.fc2 = nn.Linear(config.last_hidden, config.num_classes)\n",
    "        # self.fc1 = nn.Linear(config.dim_model, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        #return out\n",
    "        out = self.postion_embedding(out)\n",
    "        \n",
    "        for encoder in self.encoders:\n",
    "            out = encoder(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = torch.mean(out, 1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, hidden, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Multi_Head_Attention(dim_model, num_head, dropout)\n",
    "        self.feed_forward = Position_wise_Feed_Forward(dim_model, hidden, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attention(x)\n",
    "        out = self.feed_forward(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, pad_size, dropout, device):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / embed)) for i in range(embed)] for pos in range(pad_size)])\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + nn.Parameter(self.pe, requires_grad=False).to(self.device)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Scaled_Dot_Product_Attention(nn.Module):\n",
    "    '''Scaled Dot-Product Attention '''\n",
    "    def __init__(self):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, scale=None):\n",
    "        '''\n",
    "        Args:\n",
    "            Q: [batch_size, len_Q, dim_Q]\n",
    "            K: [batch_size, len_K, dim_K]\n",
    "            V: [batch_size, len_V, dim_V]\n",
    "            scale: 缩放因子 论文为根号dim_K\n",
    "        Return:\n",
    "            self-attention后的张量，以及attention张量\n",
    "        '''\n",
    "        attention = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        # if mask:  # TODO change this\n",
    "        #     attention = attention.masked_fill_(mask == 0, -1e9)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, dropout=0.0):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        assert dim_model % num_head == 0\n",
    "        self.dim_head = dim_model // self.num_head\n",
    "        self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.attention = Scaled_Dot_Product_Attention()\n",
    "        self.fc = nn.Linear(num_head * self.dim_head, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.fc_Q(x)\n",
    "        K = self.fc_K(x)\n",
    "        V = self.fc_V(x)\n",
    "        Q = Q.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        K = K.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        V = V.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        # if mask:  # TODO\n",
    "        #     mask = mask.repeat(self.num_head, 1, 1)  # TODO change this\n",
    "        scale = K.size(-1) ** -0.5  # 缩放因子\n",
    "        context = self.attention(Q, K, V, scale)\n",
    "\n",
    "        context = context.view(batch_size, -1, self.dim_head * self.num_head)\n",
    "        out = self.fc(context)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Position_wise_Feed_Forward(nn.Module):\n",
    "    def __init__(self, dim_model, hidden, dropout=0.0):\n",
    "        super(Position_wise_Feed_Forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "75ff7993-7b23-4530-99ae-3304fa57761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Config(object):\n",
    "\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Transformer'\n",
    "        self.embedding_pretrained = None                                # 预训练词向量\n",
    "        self.device = torch.device(\"mps\")                               # 设备\n",
    "        self.dropout = 0.5                                              # 随机失活\n",
    "        self.num_classes = 14                                           # 类别数\n",
    "        self.num_epochs = 10                                            # epoch数\n",
    "        self.batch_size = 128                                           # mini-batch大小\n",
    "        self.pad_size = 32                                             # 每句话处理成的长度(短填长切)\n",
    "        self.n_vocab = None                                             #这里需要读取数据的部分进行赋值\n",
    "        self.learning_rate = 5e-4                                       # 学习率\n",
    "        self.embed = 8                                                  # 词向量维度\n",
    "        self.dim_model = 8\n",
    "        self.hidden = 32\n",
    "        self.last_hidden = 16\n",
    "        self.num_head = 2\n",
    "        self.num_encoder = 1\n",
    "        self.n_splits = 2                                               #k折交叉验证\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fe8c5-4de6-4b42-a797-3ca2b32efa17",
   "metadata": {},
   "source": [
    "## load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a808a6-3f00-4379-bc74-da3fb256c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#--------------------------加载数据----------------------------\n",
    "def load_data(config):\n",
    "    df = pd.read_csv('./dataset/train_set.csv',sep='\\t')\n",
    "\n",
    "    train = []\n",
    "    targets = []\n",
    "    label = df['label'].values\n",
    "    text = df['text'].values\n",
    "    id = 0\n",
    "    vocabs_size = 0\n",
    "    for val in tqdm(text):\n",
    "        s = val.split(' ')\n",
    "        single_data = []\n",
    "        for i in range(len(s)):\n",
    "            vocabs_size = max(vocabs_size,int(s[i])+1)\n",
    "            single_data.append(int(s[i])+1)\n",
    "            if len(single_data)>=config.pad_size:\n",
    "                train.append(single_data)\n",
    "                targets.append(int(label[id]))\n",
    "                single_data = []\n",
    "        if len(single_data)>=150:\n",
    "            single_data = single_data + [0]*(config.pad_size-len(single_data))\n",
    "            train.append(single_data)\n",
    "            targets.append(int(label[id]))  \n",
    "        id += 1\n",
    "        \n",
    "\n",
    "\n",
    "    train = np.array(train)\n",
    "    targets = np.array(targets)\n",
    "    return train,targets,vocabs_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9193f-362f-476f-a8d4-5916d568542b",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "94dc749e-a8bd-4639-bc1d-f7fa9c0a3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from    torch import autograd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3e833a0-e1d0-4884-b37e-0e9490289de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [01:01<00:00, 3250.69it/s]\n"
     ]
    }
   ],
   "source": [
    "train,targets,vocabs_size = load_data(config)#加载数据\n",
    "config.n_vocab = vocabs_size + 1\n",
    "\n",
    "batch_size = config.batch_size\n",
    "\n",
    "kf = KFold(n_splits=config.n_splits, shuffle=True, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f991c-79b8-4fc0-909c-d3aa8902d78b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n",
      "开始迭代....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████| 21771/21771 [05:12<00:00, 69.66it/s, train_acc=0.478, train_loss=1.77, val_acc=0.612, val_loss=1.6]\n",
      "Training...: 100%|██████████████████████████| 21771/21771 [05:13<00:00, 69.38it/s, train_acc=0.507, train_loss=1.55, val_acc=0.627, val_loss=1.39]\n",
      "Training...: 100%|██████████████████████████| 21771/21771 [05:07<00:00, 70.70it/s, train_acc=0.537, train_loss=1.48, val_acc=0.657, val_loss=1.37]\n",
      "Training...:  66%|█████████████████▊         | 14397/21771 [03:13<01:41, 72.36it/s, train_acc=0.766, train_loss=0.9, val_acc=0.594, val_loss=1.28]"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    x_train, x_val = train[train_idx], train[test_idx]\n",
    "    y_train, y_val = targets[train_idx], targets[test_idx]\n",
    "\n",
    "    #print(x_train.shape)\n",
    "    \n",
    "    M_train = len(x_train)\n",
    "    M_val = len(x_val)\n",
    "    if M_train % batch_size == 1:#因为模型里面有层标准化，训练中不能出现单条数据，至少为2条\n",
    "        M_train -= 1\n",
    "    if M_val % batch_size == 1:\n",
    "        M_val -= 1\n",
    "    x_train = torch.from_numpy(x_train).to(torch.long).to(config.device)\n",
    "    x_val = torch.from_numpy(x_val).to(torch.long).to(config.device)\n",
    "    y_train = torch.from_numpy(y_train).to(torch.long).to(config.device)\n",
    "    y_val = torch.from_numpy(y_val).to(torch.long).to(config.device)\n",
    "\n",
    "    model = Model(config)#调用transformer的编码器\n",
    "    model.to(config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config.learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()#多分类的任务\n",
    "    model.train()\n",
    "    print('开始迭代....')\n",
    "    #开始迭代\n",
    "    for step in range(config.num_epochs):\n",
    "        #print('step=',step+1)\n",
    "        L_val = -batch_size\n",
    "        with tqdm(np.arange(0,M_train,batch_size), desc='Training...') as tbar:\n",
    "            for index in tbar:\n",
    "                L = index\n",
    "                R = min(M_train,index+batch_size)\n",
    "                L_val += batch_size\n",
    "                L_val %= M_val\n",
    "                R_val = min(M_val,L_val + batch_size)\n",
    "                #-----------------训练内容------------------\n",
    "                #print(x_train[L:R].shape)\n",
    "                train_pre = model(x_train[L:R])     # 喂给 model训练数据 x, 输出预测值\n",
    "                train_loss = loss_func(train_pre, y_train[L:R])\n",
    "                val_pre = model(x_val[L_val:R_val])#验证集也得分批次，不然数据量太大内存爆炸\n",
    "                val_loss = loss_func(val_pre, y_val[L_val:R_val])\n",
    "\n",
    "                #----------- -----计算准确率----------------\n",
    "                train_acc = np.sum(np.argmax(np.array(train_pre.data.cpu()),axis=1) == np.array(y_train[L:R].data.cpu()))/(R-L)\n",
    "                val_acc = np.sum(np.argmax(np.array(val_pre.data.cpu()),axis=1) == np.array(y_val[L_val:R_val].data.cpu()))/(R_val-L_val)\n",
    "\n",
    "                #---------------打印在进度条上--------------\n",
    "                tbar.set_postfix(train_loss=float(train_loss.data.cpu()),train_acc=train_acc,val_loss=float(val_loss.data.cpu()),val_acc=val_acc)\n",
    "                tbar.update()  # 默认参数n=1，每update一次，进度+n\n",
    "\n",
    "                #-----------------反向传播更新---------------\n",
    "                optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "                train_loss.backward()         # 以训练集的误差进行反向传播, 计算参数更新值\n",
    "                optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "569ccc79-063a-4161-aeea-0d6baed3b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(7551, 8, padding_idx=7550)\n",
      "  (postion_embedding): Positional_Encoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (attention): Multi_Head_Attention(\n",
      "      (fc_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "      (fc_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "      (fc_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "      (attention): Scaled_Dot_Product_Attention()\n",
      "      (fc): Linear(in_features=8, out_features=8, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (feed_forward): Position_wise_Feed_Forward(\n",
      "      (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "      (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (encoders): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (attention): Multi_Head_Attention(\n",
      "        (fc_Q): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (fc_K): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (fc_V): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (attention): Scaled_Dot_Product_Attention()\n",
      "        (fc): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): Position_wise_Feed_Forward(\n",
      "        (fc1): Linear(in_features=8, out_features=32, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=256, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "model = Model(config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2733a034-608d-4beb-9038-8c0f864ce3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该层的结构：[7551, 8]\n",
      "该层参数和：60408\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[32, 8]\n",
      "该层参数和：256\n",
      "该层的结构：[32]\n",
      "该层参数和：32\n",
      "该层的结构：[8, 32]\n",
      "该层参数和：256\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8, 8]\n",
      "该层参数和：64\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[32, 8]\n",
      "该层参数和：256\n",
      "该层的结构：[32]\n",
      "该层参数和：32\n",
      "该层的结构：[8, 32]\n",
      "该层参数和：256\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[8]\n",
      "该层参数和：8\n",
      "该层的结构：[14, 256]\n",
      "该层参数和：3584\n",
      "该层的结构：[14]\n",
      "该层参数和：14\n",
      "总参数数量和：65750\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de6bef-f246-427d-8d8e-56c106a18274",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73934d79-81d5-41c7-90ee-d955a2ab6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from    torch import autograd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "#--------------------------加载数据----------------------------\n",
    "df = pd.read_csv('./dataset/train_set.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23b38efd-18f0-4c0e-b14c-38d56fb6742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [01:33<00:00, 2146.70it/s]\n"
     ]
    }
   ],
   "source": [
    "#mx_length = 900\n",
    "vocabs_size = 0\n",
    "n_class = 14\n",
    "training_step = 20#迭代次数\n",
    "batch_size = 256#每个批次的大小\n",
    "train = []\n",
    "targets = []\n",
    "label = df['label'].values\n",
    "text = df['text'].values\n",
    "id = 0\n",
    "for val in tqdm(text):\n",
    "    s = val.split(' ')\n",
    "    single_data = []\n",
    "    for i in range(len(s)):\n",
    "        vocabs_size = max(vocabs_size,int(s[i])+1)\n",
    "        single_data.append(int(s[i])+1)\n",
    "        if len(single_data)>=256:\n",
    "            train.append(single_data)\n",
    "            targets.append(int(label[id]))\n",
    "            single_data = []\n",
    "    if len(single_data)>=150:\n",
    "        single_data = single_data + [0]*(256-len(single_data))\n",
    "        train.append(single_data)\n",
    "        targets.append(int(label[id]))  \n",
    "    id += 1\n",
    "    \n",
    "\n",
    "\n",
    "train = np.array(train)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d6ebb7-8118-4b4b-b4aa-d27dbfe93f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(691522, 256)\n",
      "(691522,)\n"
     ]
    }
   ],
   "source": [
    "class Bi_Lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bi_Lstm,self).__init__() \n",
    "        self.embeding = nn.Embedding(vocabs_size+1,100)\n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = 100,num_layers = 1,bidirectional = False,batch_first=True,dropout=0.5)#加了双向，输出的节点数翻2倍\n",
    "        self.l1 = nn.BatchNorm1d(100)\n",
    "        self.l2 = nn.ReLU()\n",
    "        self.l3 = nn.Linear(100,n_class)#特征输入\n",
    "        self.l4 = nn.Dropout(0.3)\n",
    "        self.l5 = nn.BatchNorm1d(n_class)\n",
    "    def forward(self, x):\n",
    "        x = self.embeding(x)\n",
    "        out,_ = self.lstm(x)\n",
    "        #选择最后一个时间点的output\n",
    "        out = self.l1(out[:,-1,:])\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.l5(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "print(train.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d1a09ae-1d35-47ea-9ba7-72b9e1fd5515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  50%|████████████████████████▉                         | 1079/2161 [01:48<01:48,  9.94it/s, train_acc=0.625, train_loss=1.36, val_acc=0.598, val_loss=1.3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m loss_func(val_pre, y_val[L_val:R_val])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#----------- -----计算准确率----------------\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39marray(train_pre\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train[L:R]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()))\u001b[38;5;241m/\u001b[39m(R\u001b[38;5;241m-\u001b[39mL)\n\u001b[1;32m     40\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39marray(val_pre\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_val[L_val:R_val]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()))\u001b[38;5;241m/\u001b[39m(R_val\u001b[38;5;241m-\u001b[39mL_val)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#---------------打印在进度条上--------------\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True, random_state=2021)#5折交叉验证\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    x_train, x_val = train[train_idx], train[test_idx]\n",
    "    y_train, y_val = targets[train_idx], targets[test_idx]\n",
    "    \n",
    "    M_train = len(x_train)-1\n",
    "    M_val = len(x_val)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).to(torch.long).to(device)\n",
    "    x_val = torch.from_numpy(x_val).to(torch.long).to(device)\n",
    "    y_train = torch.from_numpy(y_train).to(torch.long).to(device)\n",
    "    y_val = torch.from_numpy(y_val).to(torch.long).to(device)\n",
    "\n",
    "    model = Bi_Lstm()\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    loss_func = nn.CrossEntropyLoss()#多分类的任务\n",
    "    model.train()#模型中有BN和Droupout一定要添加这个说明\n",
    "    \n",
    "    #开始迭代\n",
    "    for step in range(training_step):\n",
    "        print('step=',step)\n",
    "        L_val = -batch_size\n",
    "        with tqdm(np.arange(0,M_train,batch_size), desc='Training...') as tbar:\n",
    "            for index in tbar:\n",
    "                L = index\n",
    "                R = min(M_train,index+batch_size)\n",
    "                L_val += batch_size\n",
    "                L_val %= M_val\n",
    "                R_val = min(M_val,L_val + batch_size)\n",
    "                #-----------------训练内容------------------\n",
    "                train_pre = model(x_train[L:R])     # 喂给 model训练数据 x, 输出预测值\n",
    "                train_loss = loss_func(train_pre, y_train[L:R])\n",
    "                val_pre = model(x_val[L_val:R_val])#验证集也得分批次，不然数据量太大内存爆炸\n",
    "                val_loss = loss_func(val_pre, y_val[L_val:R_val])\n",
    "\n",
    "                #----------- -----计算准确率----------------\n",
    "                train_acc = np.sum(np.argmax(np.array(train_pre.data.cpu()),axis=1) == np.array(y_train[L:R].data.cpu()))/(R-L)\n",
    "                val_acc = np.sum(np.argmax(np.array(val_pre.data.cpu()),axis=1) == np.array(y_val[L_val:R_val].data.cpu()))/(R_val-L_val)\n",
    "\n",
    "                #---------------打印在进度条上--------------\n",
    "                tbar.set_postfix(train_loss=float(train_loss.data.cpu()),train_acc=train_acc,val_loss=float(val_loss.data.cpu()),val_acc=val_acc)\n",
    "                tbar.update()  # 默认参数n=1，每update一次，进度+n\n",
    "\n",
    "                #-----------------反向传播更新---------------\n",
    "                optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "                train_loss.backward()         # 以训练集的误差进行反向传播, 计算参数更新值\n",
    "                optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095213c-30bd-4943-b7ce-e82ddff0dda7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_THU_Lab",
   "language": "python",
   "name": "ml_thu_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
