{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbac1257-6c45-4a84-ab76-46902adafee5",
   "metadata": {},
   "source": [
    "## Define TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef0bfa24-6225-49fc-920e-d9d1c9356126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, windows_size, max_len, feature_size, n_class, dropout=0.4):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # embedding层\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 卷积层特征提取\n",
    "        self.conv1 = nn.ModuleList([\n",
    "            nn.Sequential(nn.Conv1d(in_channels=embedding_dim, out_channels=feature_size, kernel_size=h),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.MaxPool1d(kernel_size=max_len-h+1),\n",
    "                          )\n",
    "            for h in windows_size]\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(feature_size*len(windows_size), n_class)\n",
    "        # dropout防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # [batch, seq_len, embed_dim]\n",
    "        x = x.permute(0, 2, 1) # [batch, embed_dim, seq_len]\n",
    "        x = [conv(x) for conv in self.conv1]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = x.view(-1, x.size(1)) # [batch, feature_size*len(windows_size)]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)# [batch, n_class]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be8f6e-8386-44cb-ab8d-43f8742484bd",
   "metadata": {},
   "source": [
    "## Train TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "faed9445-08dc-4b34-9594-5a9f0f212ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW\n",
    "import jieba\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "221c9bd4-5b75-4427-a25f-040601a1e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# 统计词库\n",
    "def count_word(sentences, word_to_index):\n",
    "    with open('./dataset/stopwords.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "    for i in range(len(stopwords)):\n",
    "        stopwords[i] = stopwords[i].strip(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in word_to_index and word not in stopwords:\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "\n",
    "# 将word转换为token\n",
    "def sentence_to_index(sentence, max_len, word_to_index):\n",
    "    sentence = [word_to_index.get(word, 0) for word in sentence]\n",
    "    if len(sentence) < max_len:\n",
    "        sentence += (max_len - len(sentence)) * [0]\n",
    "    else:\n",
    "        sentence = sentence[:max_len]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c0363da-8b55-4dad-8995-05eb3c6d2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据类\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sent, label):\n",
    "        self.sentence = np.array(sent).astype('float')\n",
    "        self.label = np.array(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label, sentence = self.label[index], self.sentence[index]\n",
    "\n",
    "        return {'label': label, 'sentence': torch.Tensor(sentence)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06431ba2-f8d5-4673-8618-df9065873930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练数据\n",
    "def load_train(path, type='char'):\n",
    "    train = {}\n",
    "    train['label'] = []\n",
    "    train['sentence'] = []\n",
    "    word_to_index = {'pad': 0}\n",
    "\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = eval(line)\n",
    "            train['label'].append(line['label'])\n",
    "            train['sentence'].append(line['sentence'])\n",
    "\n",
    "    train = pd.DataFrame(train)\n",
    "    label_to_id = {}\n",
    "    sents = []\n",
    "    labels = []\n",
    "    for i, label in enumerate(train['label'].unique()):\n",
    "        label_to_id[label] = i\n",
    "    if type == 'char':\n",
    "        train['cut_sentence'] = train['sentence']\n",
    "    else:\n",
    "        train['cut_sentence'] = train['sentence'].map(jieba.lcut)\n",
    "    count_word(train['cut_sentence'], word_to_index)\n",
    "\n",
    "    for sent, label in zip(train['cut_sentence'], train['label']):\n",
    "        sents.append(sentence_to_index(sent, max_len=max_len, word_to_index=word_to_index))\n",
    "        labels.append(label_to_id[label])\n",
    "    return sents, labels, label_to_id, word_to_index\n",
    "\n",
    "# 加载验证数据\n",
    "def load_val(val_path, label_to_id, word_to_index, type='char'):\n",
    "    val = {}\n",
    "    val['label'] = []\n",
    "    val['sentence'] = []\n",
    "    with open(val_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = eval(line)\n",
    "            val['label'].append(line['label'])\n",
    "            val['sentence'].append(line['sentence'])\n",
    "\n",
    "    val = pd.DataFrame(val)\n",
    "    val_sents = []\n",
    "    val_labels = []\n",
    "    if type == 'char':\n",
    "        val['cut_sentence'] = val['sentence']\n",
    "    else:\n",
    "        val['cut_sentence'] = val['sentence'].map(jieba.lcut)\n",
    "    for sent, label in zip(val['cut_sentence'], val['label']):\n",
    "        val_sents.append(sentence_to_index(sent, max_len=max_len, word_to_index=word_to_index))\n",
    "        val_labels.append(label_to_id[label])\n",
    "\n",
    "    return val_sents, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea1846ec-c644-4183-9dc7-e6705786cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "def evaluate(val_loader, model, device):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for data in val_loader:\n",
    "        label = data['label']\n",
    "        sentence = data['sentence']\n",
    "\n",
    "        label = label.type(torch.LongTensor)\n",
    "        sentence = sentence.type(torch.LongTensor)\n",
    "\n",
    "        label = label.to(device)\n",
    "        sentence = sentence.to(device)\n",
    "\n",
    "        output = model(sentence)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (output.argmax(1) == label).sum().item()\n",
    "\n",
    "    size = len(val_dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = corrects / size\n",
    "    print('\\nEvaluation - loss: {:.3f} acc: {:.4f}\\n'.format(avg_loss, accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df84c91d-db26-4531-9ae4-d670f8250e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_torch()\n",
    "train_path = './dataset/tnews_public/train.json'\n",
    "val_path = './dataset/tnews_public/dev.json'\n",
    "max_len = 64\n",
    "# 加载数据\n",
    "sents, labels, label_to_id, word_to_index = load_train(train_path, type='char')\n",
    "val_sents, val_labels = load_val(val_path, label_to_id, word_to_index, type='char')\n",
    "# 超参数\n",
    "batch_size = 128\n",
    "learn_rate = 3e-4\n",
    "n_epochs = 64\n",
    "embedding_dim = 300\n",
    "windows_size = [2, 4, 3]\n",
    "feature_size = 100\n",
    "dropout = 0.5\n",
    "vocab_size = len(word_to_index)\n",
    "n_class = len(label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2443385b-2704-4421-9b7d-f34862f27362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch: 100/417, loss: 0.0414\n",
      "epoch: 1, batch: 200/417, loss: 0.0369\n",
      "epoch: 1, batch: 300/417, loss: 0.0342\n",
      "epoch: 1, batch: 400/417, loss: 0.0324\n",
      "\n",
      "Evaluation - loss: 0.015 acc: 0.4347\n",
      "\n",
      "epoch: 2, batch: 100/417, loss: 0.0302\n",
      "epoch: 2, batch: 200/417, loss: 0.0295\n",
      "epoch: 2, batch: 300/417, loss: 0.0289\n",
      "epoch: 2, batch: 400/417, loss: 0.0282\n",
      "\n",
      "Evaluation - loss: 0.013 acc: 0.4751\n",
      "\n",
      "epoch: 3, batch: 100/417, loss: 0.0269\n",
      "epoch: 3, batch: 200/417, loss: 0.0265\n",
      "epoch: 3, batch: 300/417, loss: 0.0266\n",
      "epoch: 3, batch: 400/417, loss: 0.0265\n",
      "\n",
      "Evaluation - loss: 0.013 acc: 0.4902\n",
      "\n",
      "epoch: 4, batch: 100/417, loss: 0.0247\n",
      "epoch: 4, batch: 200/417, loss: 0.0252\n",
      "epoch: 4, batch: 300/417, loss: 0.025\n",
      "epoch: 4, batch: 400/417, loss: 0.0254\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.4992\n",
      "\n",
      "epoch: 5, batch: 100/417, loss: 0.0239\n",
      "epoch: 5, batch: 200/417, loss: 0.0237\n",
      "epoch: 5, batch: 300/417, loss: 0.0235\n",
      "epoch: 5, batch: 400/417, loss: 0.024\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5053\n",
      "\n",
      "epoch: 6, batch: 100/417, loss: 0.0225\n",
      "epoch: 6, batch: 200/417, loss: 0.0227\n",
      "epoch: 6, batch: 300/417, loss: 0.0229\n",
      "epoch: 6, batch: 400/417, loss: 0.0229\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5099\n",
      "\n",
      "epoch: 7, batch: 100/417, loss: 0.0213\n",
      "epoch: 7, batch: 200/417, loss: 0.022\n",
      "epoch: 7, batch: 300/417, loss: 0.0219\n",
      "epoch: 7, batch: 400/417, loss: 0.0222\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5147\n",
      "\n",
      "epoch: 8, batch: 100/417, loss: 0.0206\n",
      "epoch: 8, batch: 200/417, loss: 0.0209\n",
      "epoch: 8, batch: 300/417, loss: 0.021\n",
      "epoch: 8, batch: 400/417, loss: 0.0213\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5175\n",
      "\n",
      "epoch: 9, batch: 100/417, loss: 0.0197\n",
      "epoch: 9, batch: 200/417, loss: 0.0201\n",
      "epoch: 9, batch: 300/417, loss: 0.0202\n",
      "epoch: 9, batch: 400/417, loss: 0.0205\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5211\n",
      "\n",
      "epoch: 10, batch: 100/417, loss: 0.019\n",
      "epoch: 10, batch: 200/417, loss: 0.0191\n",
      "epoch: 10, batch: 300/417, loss: 0.0193\n",
      "epoch: 10, batch: 400/417, loss: 0.0199\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5202\n",
      "\n",
      "epoch: 11, batch: 100/417, loss: 0.0181\n",
      "epoch: 11, batch: 200/417, loss: 0.0185\n",
      "epoch: 11, batch: 300/417, loss: 0.0189\n",
      "epoch: 11, batch: 400/417, loss: 0.0189\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5265\n",
      "\n",
      "epoch: 12, batch: 100/417, loss: 0.0175\n",
      "epoch: 12, batch: 200/417, loss: 0.0178\n",
      "epoch: 12, batch: 300/417, loss: 0.0181\n",
      "epoch: 12, batch: 400/417, loss: 0.0182\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5250\n",
      "\n",
      "epoch: 13, batch: 100/417, loss: 0.017\n",
      "epoch: 13, batch: 200/417, loss: 0.0173\n",
      "epoch: 13, batch: 300/417, loss: 0.0174\n",
      "epoch: 13, batch: 400/417, loss: 0.0178\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5233\n",
      "\n",
      "epoch: 14, batch: 100/417, loss: 0.0161\n",
      "epoch: 14, batch: 200/417, loss: 0.0163\n",
      "epoch: 14, batch: 300/417, loss: 0.0169\n",
      "epoch: 14, batch: 400/417, loss: 0.0169\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5260\n",
      "\n",
      "epoch: 15, batch: 100/417, loss: 0.0156\n",
      "epoch: 15, batch: 200/417, loss: 0.0158\n",
      "epoch: 15, batch: 300/417, loss: 0.0164\n",
      "epoch: 15, batch: 400/417, loss: 0.0166\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5270\n",
      "\n",
      "epoch: 16, batch: 100/417, loss: 0.0148\n",
      "epoch: 16, batch: 200/417, loss: 0.0154\n",
      "epoch: 16, batch: 300/417, loss: 0.0156\n",
      "epoch: 16, batch: 400/417, loss: 0.0161\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5263\n",
      "\n",
      "epoch: 17, batch: 100/417, loss: 0.0146\n",
      "epoch: 17, batch: 200/417, loss: 0.015\n",
      "epoch: 17, batch: 300/417, loss: 0.0152\n",
      "epoch: 17, batch: 400/417, loss: 0.0153\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5293\n",
      "\n",
      "epoch: 18, batch: 100/417, loss: 0.0141\n",
      "epoch: 18, batch: 200/417, loss: 0.0144\n",
      "epoch: 18, batch: 300/417, loss: 0.0148\n",
      "epoch: 18, batch: 400/417, loss: 0.0148\n",
      "\n",
      "Evaluation - loss: 0.011 acc: 0.5288\n",
      "\n",
      "epoch: 19, batch: 100/417, loss: 0.0135\n",
      "epoch: 19, batch: 200/417, loss: 0.0138\n",
      "epoch: 19, batch: 300/417, loss: 0.0141\n",
      "epoch: 19, batch: 400/417, loss: 0.0145\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5286\n",
      "\n",
      "epoch: 20, batch: 100/417, loss: 0.013\n",
      "epoch: 20, batch: 200/417, loss: 0.0132\n",
      "epoch: 20, batch: 300/417, loss: 0.0138\n",
      "epoch: 20, batch: 400/417, loss: 0.0141\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5307\n",
      "\n",
      "epoch: 21, batch: 100/417, loss: 0.0127\n",
      "epoch: 21, batch: 200/417, loss: 0.013\n",
      "epoch: 21, batch: 300/417, loss: 0.0132\n",
      "epoch: 21, batch: 400/417, loss: 0.0135\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5307\n",
      "\n",
      "epoch: 22, batch: 100/417, loss: 0.0123\n",
      "epoch: 22, batch: 200/417, loss: 0.0127\n",
      "epoch: 22, batch: 300/417, loss: 0.0131\n",
      "epoch: 22, batch: 400/417, loss: 0.0132\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5285\n",
      "\n",
      "epoch: 23, batch: 100/417, loss: 0.0117\n",
      "epoch: 23, batch: 200/417, loss: 0.0121\n",
      "epoch: 23, batch: 300/417, loss: 0.0125\n",
      "epoch: 23, batch: 400/417, loss: 0.013\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5295\n",
      "\n",
      "epoch: 24, batch: 100/417, loss: 0.0116\n",
      "epoch: 24, batch: 200/417, loss: 0.0117\n",
      "epoch: 24, batch: 300/417, loss: 0.0121\n",
      "epoch: 24, batch: 400/417, loss: 0.0127\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5297\n",
      "\n",
      "epoch: 25, batch: 100/417, loss: 0.0112\n",
      "epoch: 25, batch: 200/417, loss: 0.0116\n",
      "epoch: 25, batch: 300/417, loss: 0.0121\n",
      "epoch: 25, batch: 400/417, loss: 0.0121\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5282\n",
      "\n",
      "epoch: 26, batch: 100/417, loss: 0.0108\n",
      "epoch: 26, batch: 200/417, loss: 0.0113\n",
      "epoch: 26, batch: 300/417, loss: 0.0115\n",
      "epoch: 26, batch: 400/417, loss: 0.0118\n",
      "\n",
      "Evaluation - loss: 0.012 acc: 0.5275\n",
      "\n",
      "EarlyStopping---best_acc: 0.5307, \n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "train_dataset = MyDataset(sents, labels)\n",
    "val_dataset = MyDataset(val_sents, val_labels)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0)\n",
    "\n",
    "device = torch.device(\"mps\" if (torch.backends.mps.is_available()) else \"cpu\")\n",
    "model = TextCNN(vocab_size, embedding_dim, windows_size, max_len, feature_size, n_class, dropout).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learn_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_acc = 0.0\n",
    "early_times = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    for batch_i, data in enumerate(train_loader):\n",
    "        start = time.time()\n",
    "        label = data['label']\n",
    "        sentence = data['sentence']\n",
    "\n",
    "        label = label.type(torch.LongTensor)\n",
    "        sentence = sentence.type(torch.LongTensor)\n",
    "\n",
    "        label = label.to(device)\n",
    "        sentence = sentence.to(device)\n",
    "\n",
    "        output = model(sentence)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch_i % 100 == 99:\n",
    "            print(\n",
    "                'epoch: {}, batch: {}/{}, loss: {}'.format(epoch, batch_i + 1, len(train_loader),\n",
    "                                                           round(running_loss / 100/64, 4)))\n",
    "            running_loss = 0.0\n",
    "    val_acc = evaluate(val_loader, model, device)\n",
    "    if best_acc < val_acc:\n",
    "        best_acc = val_acc\n",
    "        early_times = 0\n",
    "    else:\n",
    "        early_times += 1\n",
    "        if early_times > 5:\n",
    "            print('EarlyStopping---best_acc: {}, '.format(best_acc))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b6ef438-f410-45b9-b62d-b34e9b9da590",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(params\u001b[38;5;241m.\u001b[39mshape())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "#print(params.shape)\n",
    "#print(\"shape of weight: \", np.array(params[0]).shape)\n",
    "#print(\"shape of bias: \", np.array(params[1]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b6000-9d45-49bc-b106-4e138863d5f8",
   "metadata": {},
   "source": [
    "# Transformer&Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab43f15-71f4-4c22-ba63-f91d4de74c7b",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26fda26c-bd72-43e3-97fb-1cec2a0b447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "'''Attention Is All You Need'''\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        if config.embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)\n",
    "\n",
    "        self.postion_embedding = Positional_Encoding(config.embed, config.pad_size, config.dropout, config.device)\n",
    "        self.encoder = Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            copy.deepcopy(self.encoder)\n",
    "            # Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "            for _ in range(config.num_encoder)])\n",
    "\n",
    "        self.fc1 = nn.Linear(config.pad_size * config.dim_model, config.num_classes)\n",
    "        # self.fc2 = nn.Linear(config.last_hidden, config.num_classes)\n",
    "        # self.fc1 = nn.Linear(config.dim_model, config.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        #return out\n",
    "        out = self.postion_embedding(out)\n",
    "        for encoder in self.encoders:\n",
    "            out = encoder(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = torch.mean(out, 1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, hidden, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Multi_Head_Attention(dim_model, num_head, dropout)\n",
    "        self.feed_forward = Position_wise_Feed_Forward(dim_model, hidden, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attention(x)\n",
    "        out = self.feed_forward(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, pad_size, dropout, device):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / embed)) for i in range(embed)] for pos in range(pad_size)])\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + nn.Parameter(self.pe, requires_grad=False).to(self.device)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Scaled_Dot_Product_Attention(nn.Module):\n",
    "    '''Scaled Dot-Product Attention '''\n",
    "    def __init__(self):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, scale=None):\n",
    "        '''\n",
    "        Args:\n",
    "            Q: [batch_size, len_Q, dim_Q]\n",
    "            K: [batch_size, len_K, dim_K]\n",
    "            V: [batch_size, len_V, dim_V]\n",
    "            scale: 缩放因子 论文为根号dim_K\n",
    "        Return:\n",
    "            self-attention后的张量，以及attention张量\n",
    "        '''\n",
    "        attention = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        # if mask:  # TODO change this\n",
    "        #     attention = attention.masked_fill_(mask == 0, -1e9)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, dropout=0.0):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        assert dim_model % num_head == 0\n",
    "        self.dim_head = dim_model // self.num_head\n",
    "        self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.attention = Scaled_Dot_Product_Attention()\n",
    "        self.fc = nn.Linear(num_head * self.dim_head, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.fc_Q(x)\n",
    "        K = self.fc_K(x)\n",
    "        V = self.fc_V(x)\n",
    "        Q = Q.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        K = K.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        V = V.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        # if mask:  # TODO\n",
    "        #     mask = mask.repeat(self.num_head, 1, 1)  # TODO change this\n",
    "        scale = K.size(-1) ** -0.5  # 缩放因子\n",
    "        context = self.attention(Q, K, V, scale)\n",
    "\n",
    "        context = context.view(batch_size, -1, self.dim_head * self.num_head)\n",
    "        out = self.fc(context)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Position_wise_Feed_Forward(nn.Module):\n",
    "    def __init__(self, dim_model, hidden, dropout=0.0):\n",
    "        super(Position_wise_Feed_Forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75ff7993-7b23-4530-99ae-3304fa57761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Config(object):\n",
    "\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = 'Transformer'\n",
    "        self.embedding_pretrained = None                                 # 预训练词向量\n",
    "        self.device = torch.device(\"mps\" if (torch.backends.mps.is_available()) else 'cpu')   # 设备\n",
    "        self.dropout = 0.5                                              # 随机失活\n",
    "        self.num_classes = 14                        # 类别数\n",
    "        self.num_epochs = 20                                            # epoch数\n",
    "        self.batch_size = 128                                           # mini-batch大小\n",
    "        self.pad_size = 256                                              # 每句话处理成的长度(短填长切)\n",
    "        self.n_vocab = None#这里需要读取数据的部分进行赋值\n",
    "        self.learning_rate = 5e-4                                       # 学习率\n",
    "        self.embed = 90           # 词向量维度\n",
    "        self.dim_model = 90\n",
    "        self.hidden = 1024\n",
    "        self.last_hidden = 512\n",
    "        self.num_head = 5\n",
    "        self.num_encoder = 2\n",
    "        self.n_splits = 5#k折交叉验证\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fe8c5-4de6-4b42-a797-3ca2b32efa17",
   "metadata": {},
   "source": [
    "## load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92a808a6-3f00-4379-bc74-da3fb256c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#--------------------------加载数据----------------------------\n",
    "def load_data(config):\n",
    "    df = pd.read_csv('./dataset/train_set.csv',sep='\\t')\n",
    "\n",
    "    train = []\n",
    "    targets = []\n",
    "    label = df['label'].values\n",
    "    text = df['text'].values\n",
    "    id = 0\n",
    "    vocabs_size = 0\n",
    "    for val in tqdm(text):\n",
    "        s = val.split(' ')\n",
    "        single_data = []\n",
    "        for i in range(len(s)):\n",
    "            vocabs_size = max(vocabs_size,int(s[i])+1)\n",
    "            single_data.append(int(s[i])+1)\n",
    "            if len(single_data)>=config.pad_size:\n",
    "                train.append(single_data)\n",
    "                targets.append(int(label[id]))\n",
    "                single_data = []\n",
    "        if len(single_data)>=150:\n",
    "            single_data = single_data + [0]*(config.pad_size-len(single_data))\n",
    "            train.append(single_data)\n",
    "            targets.append(int(label[id]))  \n",
    "        id += 1\n",
    "        \n",
    "\n",
    "\n",
    "    train = np.array(train)\n",
    "    targets = np.array(targets)\n",
    "    return train,targets,vocabs_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9193f-362f-476f-a8d4-5916d568542b",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6569a5-6f1b-4ac3-a853-66ae391833d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [00:59<00:00, 3368.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n",
      "开始迭代....\n",
      "step= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [16:46<00:00,  4.29it/s, train_acc=0.852, train_loss=0.478, val_acc=0.781, val_loss=0.663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [14:48<00:00,  4.86it/s, train_acc=0.875, train_loss=0.362, val_acc=0.828, val_loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|████████████████████████████████████████████████| 4322/4322 [14:48<00:00,  4.86it/s, train_acc=0.953, train_loss=0.23, val_acc=0.859, val_loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [14:49<00:00,  4.86it/s, train_acc=0.922, train_loss=0.251, val_acc=0.836, val_loss=0.453]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [14:55<00:00,  4.83it/s, train_acc=0.961, train_loss=0.194, val_acc=0.844, val_loss=0.464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [15:00<00:00,  4.80it/s, train_acc=0.953, train_loss=0.166, val_acc=0.859, val_loss=0.412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|████████████████████████████████████████████████| 4322/4322 [19:10<00:00,  3.76it/s, train_acc=0.93, train_loss=0.199, val_acc=0.852, val_loss=0.485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [18:53<00:00,  3.81it/s, train_acc=0.914, train_loss=0.214, val_acc=0.883, val_loss=0.377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|███████████████████████████████████████████████| 4322/4322 [47:08<00:00,  1.53it/s, train_acc=0.961, train_loss=0.159, val_acc=0.852, val_loss=0.419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  82%|██████████████████████████████████████▋        | 3559/4322 [11:55<02:36,  4.86it/s, train_acc=0.867, train_loss=0.355, val_acc=0.828, val_loss=0.488]"
     ]
    }
   ],
   "source": [
    "\n",
    "#---------------------------------------------------\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from    torch import autograd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = Config()\n",
    "train,targets,vocabs_size = load_data(config)#加载数据\n",
    "config.n_vocab = vocabs_size + 1\n",
    "\n",
    "batch_size = config.batch_size\n",
    "\n",
    "kf = KFold(n_splits=config.n_splits, shuffle=True, random_state=2021)\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "    x_train, x_val = train[train_idx], train[test_idx]\n",
    "    y_train, y_val = targets[train_idx], targets[test_idx]\n",
    "    \n",
    "    M_train = len(x_train)\n",
    "    M_val = len(x_val)\n",
    "    if M_train % batch_size == 1:#因为模型里面有层标准化，训练中不能出现单条数据，至少为2条\n",
    "        M_train -= 1\n",
    "    if M_val % batch_size == 1:\n",
    "        M_val -= 1\n",
    "    x_train = torch.from_numpy(x_train).to(torch.long).to(config.device)\n",
    "    x_val = torch.from_numpy(x_val).to(torch.long).to(config.device)\n",
    "    y_train = torch.from_numpy(y_train).to(torch.long).to(config.device)\n",
    "    y_val = torch.from_numpy(y_val).to(torch.long).to(config.device)\n",
    "\n",
    "    model = Model(config)#调用transformer的编码器\n",
    "    model.to(config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config.learning_rate)\n",
    "    loss_func = nn.CrossEntropyLoss()#多分类的任务\n",
    "    model.train()\n",
    "    print('开始迭代....')\n",
    "    #开始迭代\n",
    "    for step in range(config.num_epochs):\n",
    "        print('step=',step+1)\n",
    "        L_val = -batch_size\n",
    "        with tqdm(np.arange(0,M_train,batch_size), desc='Training...') as tbar:\n",
    "            for index in tbar:\n",
    "                L = index\n",
    "                R = min(M_train,index+batch_size)\n",
    "                L_val += batch_size\n",
    "                L_val %= M_val\n",
    "                R_val = min(M_val,L_val + batch_size)\n",
    "                #-----------------训练内容------------------\n",
    "                train_pre = model(x_train[L:R])     # 喂给 model训练数据 x, 输出预测值\n",
    "                train_loss = loss_func(train_pre, y_train[L:R])\n",
    "                val_pre = model(x_val[L_val:R_val])#验证集也得分批次，不然数据量太大内存爆炸\n",
    "                val_loss = loss_func(val_pre, y_val[L_val:R_val])\n",
    "\n",
    "                #----------- -----计算准确率----------------\n",
    "                train_acc = np.sum(np.argmax(np.array(train_pre.data.cpu()),axis=1) == np.array(y_train[L:R].data.cpu()))/(R-L)\n",
    "                val_acc = np.sum(np.argmax(np.array(val_pre.data.cpu()),axis=1) == np.array(y_val[L_val:R_val].data.cpu()))/(R_val-L_val)\n",
    "\n",
    "                #---------------打印在进度条上--------------\n",
    "                tbar.set_postfix(train_loss=float(train_loss.data.cpu()),train_acc=train_acc,val_loss=float(val_loss.data.cpu()),val_acc=val_acc)\n",
    "                tbar.update()  # 默认参数n=1，每update一次，进度+n\n",
    "\n",
    "                #-----------------反向传播更新---------------\n",
    "                optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "                train_loss.backward()         # 以训练集的误差进行反向传播, 计算参数更新值\n",
    "                optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76782b5a-5651-4346-b3d1-25f4f0027d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_THU_Lab",
   "language": "python",
   "name": "ml_thu_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
